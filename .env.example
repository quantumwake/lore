# Inference API Configuration
# For Ollama (default)
INFERENCE_API_URL=http://localhost:11434

# For OpenAI-compatible endpoints
# INFERENCE_API_URL=https://api.openai.com/v1
# INFERENCE_API_KEY=your-api-key-here

# MCP Server Configuration
MCP_SERVER_URL=http://localhost:3000